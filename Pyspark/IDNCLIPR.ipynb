{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `row_name` cannot be resolved. Did you mean one of the following? [`row_number`, `GRUP_ECO`, `ROL_JERA`, `IDNUMCLI`].;\n'Project [IDNUMCLI#780L, GRUP_ECO#781L, ROL_JERA#782L, row_number#787, CASE WHEN ('row_name = 1) THEN IDNUMCLI ELSE collect_list(IDNUMCLI#780L, 0, 0) windowspecdefinition(GRUP_ECO#781L, ROL_JERA#782L, GRUP_ECO#781L ASC NULLS FIRST, ROL_JERA#782L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) END AS temp#793]\n+- Project [IDNUMCLI#780L, GRUP_ECO#781L, ROL_JERA#782L, row_number#787]\n   +- Project [IDNUMCLI#780L, GRUP_ECO#781L, ROL_JERA#782L, row_number#787, row_number#787]\n      +- Window [row_number() windowspecdefinition(GRUP_ECO#781L, ROL_JERA#782L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_number#787], [GRUP_ECO#781L], [ROL_JERA#782L ASC NULLS FIRST]\n         +- Project [IDNUMCLI#780L, GRUP_ECO#781L, ROL_JERA#782L]\n            +- LogicalRDD [IDNUMCLI#780L, GRUP_ECO#781L, ROL_JERA#782L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m df_with_row_number \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow_number\u001b[39m\u001b[38;5;124m\"\u001b[39m, row_number()\u001b[38;5;241m.\u001b[39mover(windowSpec2))\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Adicione uma coluna para concatenar os valores\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m df_concatenated \u001b[38;5;241m=\u001b[39m \u001b[43mdf_with_row_number\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrow_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIDNUMCLI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43motherwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollect_list\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIDNUMCLI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mover\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindowSpec\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m     38\u001b[0m                     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIDNCLIPR\u001b[39m\u001b[38;5;124m\"\u001b[39m, concat_ws(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \\\n\u001b[1;32m     39\u001b[0m                     \u001b[38;5;66;03m#.filter((col(\"temp\").isNotNull()) & ((col(\"GRUP_ECO\") != first(\"GRUP_ECO\").over(Window.partitionBy(\"ROL_JERA\").orderBy(\"GRUP_ECO\"))) | (col(\"ROL_JERA\") != first(\"ROL_JERA\").over(Window.partitionBy(\"GRUP_ECO\").orderBy(\"ROL_JERA\"))))) \\\u001b[39;00m\n\u001b[1;32m     40\u001b[0m                     \u001b[38;5;66;03m#.drop(\"temp\")\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Mostrar DataFrame resultante\u001b[39;00m\n\u001b[1;32m     43\u001b[0m df_concatenated\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:5174\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   5170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   5171\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5172\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   5173\u001b[0m     )\n\u001b[0;32m-> 5174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `row_name` cannot be resolved. Did you mean one of the following? [`row_number`, `GRUP_ECO`, `ROL_JERA`, `IDNUMCLI`].;\n'Project [IDNUMCLI#780L, GRUP_ECO#781L, ROL_JERA#782L, row_number#787, CASE WHEN ('row_name = 1) THEN IDNUMCLI ELSE collect_list(IDNUMCLI#780L, 0, 0) windowspecdefinition(GRUP_ECO#781L, ROL_JERA#782L, GRUP_ECO#781L ASC NULLS FIRST, ROL_JERA#782L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) END AS temp#793]\n+- Project [IDNUMCLI#780L, GRUP_ECO#781L, ROL_JERA#782L, row_number#787]\n   +- Project [IDNUMCLI#780L, GRUP_ECO#781L, ROL_JERA#782L, row_number#787, row_number#787]\n      +- Window [row_number() windowspecdefinition(GRUP_ECO#781L, ROL_JERA#782L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_number#787], [GRUP_ECO#781L], [ROL_JERA#782L ASC NULLS FIRST]\n         +- Project [IDNUMCLI#780L, GRUP_ECO#781L, ROL_JERA#782L]\n            +- LogicalRDD [IDNUMCLI#780L, GRUP_ECO#781L, ROL_JERA#782L], false\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Inicialize a sessão Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SAS to PySpark Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Crie um DataFrame de exemplo\n",
    "data = [\n",
    "    (1, 500, 2),\n",
    "    (2, 500, 3),\n",
    "    (3, 500, 5),\n",
    "    (4, 501, 3),\n",
    "    (5, 502, 5),\n",
    "    (6, 503, 2),\n",
    "    (7, 503, 3),\n",
    "    (8, 503, 5),\n",
    "    (9, 504, 2),\n",
    "    (10, 504, 2),\n",
    "    (11, 504, 3),\n",
    "    (12, 504, 3),\n",
    "    (13, 504, 5)    \n",
    "]\n",
    "columns = [\"IDNUMCLI\", \"GRUP_ECO\", \"ROL_JERA\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Defina a janela de partição\n",
    "windowSpec1 = Window.partitionBy(\"GRUP_ECO\",\"ROL_JERA\").orderBy(\"GRUP_ECO\",\"ROL_JERA\")\n",
    "windowSpec2 = Window.partitionBy(\"GRUP_ECO\").orderBy(\"ROL_JERA\")\n",
    "\n",
    "df_with_row_number = df.withColumn(\"row_number\", row_number().over(windowSpec2))\n",
    "\n",
    "# Adicione uma coluna para concatenar os valores\n",
    "df_concatenated = df_with_row_number.withColumn(\"temp\", when(col(\"row_name\")==1, \"IDNUMCLI\")\n",
    "                                                        .otherwise(collect_list(\"IDNUMCLI\").over(windowSpec))) \\\n",
    "                    .withColumn(\"IDNCLIPR\", concat_ws(\",\", \"temp\")) \\\n",
    "                    #.filter((col(\"temp\").isNotNull()) & ((col(\"GRUP_ECO\") != first(\"GRUP_ECO\").over(Window.partitionBy(\"ROL_JERA\").orderBy(\"GRUP_ECO\"))) | (col(\"ROL_JERA\") != first(\"ROL_JERA\").over(Window.partitionBy(\"GRUP_ECO\").orderBy(\"ROL_JERA\"))))) \\\n",
    "                    #.drop(\"temp\")\n",
    "\n",
    "# Mostrar DataFrame resultante\n",
    "df_concatenated.show()\n",
    "\n",
    "# Encerrar a sessão Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+----------+-------------------+\n",
      "|IDNUMCLI|GRUP_ECO|ROL_JERA|row_number|               temp|\n",
      "+--------+--------+--------+----------+-------------------+\n",
      "|       1|     500|       2|         1|          [1, 2, 3]|\n",
      "|       2|     500|       3|         2|          [1, 2, 3]|\n",
      "|       3|     500|       5|         3|          [1, 2, 3]|\n",
      "|       4|     501|       3|         1|                [4]|\n",
      "|       5|     502|       5|         1|                [5]|\n",
      "|       6|     503|       2|         1|          [6, 7, 8]|\n",
      "|       7|     503|       3|         2|          [6, 7, 8]|\n",
      "|       8|     503|       5|         3|          [6, 7, 8]|\n",
      "|       9|     504|       2|         1|[9, 10, 11, 12, 13]|\n",
      "|      10|     504|       2|         2|[9, 10, 11, 12, 13]|\n",
      "|      11|     504|       3|         3|[9, 10, 11, 12, 13]|\n",
      "|      12|     504|       3|         4|[9, 10, 11, 12, 13]|\n",
      "|      13|     504|       5|         5|[9, 10, 11, 12, 13]|\n",
      "+--------+--------+--------+----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Defina a janela de partição\n",
    "windowSpec1 = Window.partitionBy(\"GRUP_ECO\",\"ROL_JERA\").orderBy(\"GRUP_ECO\",\"ROL_JERA\")\n",
    "windowSpec2 = Window.partitionBy(\"GRUP_ECO\").orderBy(\"ROL_JERA\")\n",
    "windowSpec3 = Window.partitionBy(\"GRUP_ECO\").orderBy(\"GRUP_ECO\")\n",
    "\n",
    "df_concatenated = df_with_row_number.withColumn(\"temp\", collect_list(\"IDNUMCLI\").over(windowSpec3))\n",
    "df_concatenated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Inicialize a sessão Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SAS to PySpark Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Crie um DataFrame de exemplo\n",
    "data = [\n",
    "    (1, 500, 2),\n",
    "    (2, 500, 3),\n",
    "    (3, 500, 5),\n",
    "    (4, 501, 3),\n",
    "    (5, 502, 5),\n",
    "    (6, 503, 2),\n",
    "    (7, 503, 3),\n",
    "    (8, 503, 5),\n",
    "    (9, 504, 2),\n",
    "    (10, 504, 2),\n",
    "    (11, 504, 3),\n",
    "    (12, 504, 3),\n",
    "    (13, 504, 5)    \n",
    "]\n",
    "columns = [\"IDNUMCLI\", \"GRUP_ECO\", \"ROL_JERA\"]\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDNUMCLI</th>\n",
       "      <th>GRUP_ECO</th>\n",
       "      <th>ROL_JERA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>501</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>502</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>503</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>503</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>503</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>504</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>504</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>504</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>504</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>504</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    IDNUMCLI  GRUP_ECO  ROL_JERA\n",
       "0          1       500         2\n",
       "1          2       500         3\n",
       "2          3       500         5\n",
       "3          4       501         3\n",
       "4          5       502         5\n",
       "5          6       503         2\n",
       "6          7       503         3\n",
       "7          8       503         5\n",
       "8          9       504         2\n",
       "9         10       504         2\n",
       "10        11       504         3\n",
       "11        12       504         3\n",
       "12        13       504         5"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDNUMCLI</th>\n",
       "      <th>GRUP_ECO</th>\n",
       "      <th>ROL_JERA</th>\n",
       "      <th>IDNCLIPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>501</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>502</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>503</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>503</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>503</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>504</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>504</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>504</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>504</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>504</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    IDNUMCLI  GRUP_ECO  ROL_JERA IDNCLIPR\n",
       "0          1       500         2         \n",
       "1          2       500         3        1\n",
       "2          3       500         5        3\n",
       "3          4       501         3        4\n",
       "4          5       502         5        5\n",
       "5          6       503         2         \n",
       "6          7       503         3        6\n",
       "7          8       503         5        8\n",
       "8          9       504         2         \n",
       "9         10       504         2        9\n",
       "10        11       504         3       10\n",
       "11        12       504         3       11\n",
       "12        13       504         5       13"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar a tabela anterior do Pandas\n",
    "df_tabela_anterior = df # Substitua pelo caminho correto do arquivo\n",
    "\n",
    "# Ordenar o DataFrame pelo GRUP_ECO e ROL_JERA\n",
    "df_tabela_anterior.sort_values(by=[\"GRUP_ECO\", \"ROL_JERA\"], inplace=True)\n",
    "\n",
    "# Inicializar a coluna IDNCLIPR com uma string vazia\n",
    "df_tabela_anterior[\"IDNCLIPR\"] = \"\"\n",
    "\n",
    "# Agrupar o DataFrame por GRUP_ECO\n",
    "for group, data in df_tabela_anterior.groupby(\"GRUP_ECO\"):\n",
    "    temp = []  # Lista temporária para armazenar os valores IDNUMCLI\n",
    "    for idx, row in data.iterrows():\n",
    "        if len(temp) > 0 and row[\"ROL_JERA\"] != temp[-1]:  # Se for o primeiro ROL_JERA diferente\n",
    "            df_tabela_anterior.loc[idx, \"IDNCLIPR\"] = \",\".join(str(val) for val in temp)  # Concatenar os valores IDNUMCLI\n",
    "            temp = []  # Limpar a lista temporária\n",
    "        temp.append(row[\"IDNUMCLI\"])  # Adicionar o IDNUMCLI à lista temporária\n",
    "    if len(temp) > 0:  # Atualizar a última linha do grupo\n",
    "        last_idx = data.index[-1]\n",
    "        df_tabela_anterior.loc[last_idx, \"IDNCLIPR\"] = \",\".join(str(val) for val in temp)\n",
    "\n",
    "# Salvar o DataFrame resultante\n",
    "df_tabela_anterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/18 14:45:23 ERROR Executor: Exception in task 0.0 in stage 11.0 (TID 35)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2484272/3170025621.py\", line 32, in cumulative_sum\n",
      "  File \"/home/Matheus/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/Matheus/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 866, in sum\n",
      "    return _invoke_function_over_columns(\"sum\", col)\n",
      "  File \"/home/Matheus/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 105, in _invoke_function_over_columns\n",
      "    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n",
      "  File \"/home/Matheus/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 105, in <genexpr>\n",
      "    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n",
      "  File \"/home/Matheus/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 65, in _to_java_column\n",
      "    raise PySparkTypeError(\n",
      "pyspark.errors.exceptions.base.PySparkTypeError: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got int.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/04/18 14:45:23 WARN TaskSetManager: Lost task 0.0 in stage 11.0 (TID 35) (fedora executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2484272/3170025621.py\", line 32, in cumulative_sum\n",
      "  File \"/home/Matheus/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/Matheus/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 866, in sum\n",
      "    return _invoke_function_over_columns(\"sum\", col)\n",
      "  File \"/home/Matheus/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 105, in _invoke_function_over_columns\n",
      "    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n",
      "  File \"/home/Matheus/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 105, in <genexpr>\n",
      "    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n",
      "  File \"/home/Matheus/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 65, in _to_java_column\n",
      "    raise PySparkTypeError(\n",
      "pyspark.errors.exceptions.base.PySparkTypeError: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got int.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/04/18 14:45:23 ERROR TaskSetManager: Task 0 in stage 11.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_2484272/3170025621.py\", line 32, in cumulative_sum\n  File \"/home/Matheus/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\n  File \"/home/Matheus/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 866, in sum\n    return _invoke_function_over_columns(\"sum\", col)\n  File \"/home/Matheus/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 105, in _invoke_function_over_columns\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n  File \"/home/Matheus/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 105, in <genexpr>\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n  File \"/home/Matheus/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 65, in _to_java_column\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got int.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m df_with_cumulative_sum \u001b[38;5;241m=\u001b[39m df_with_cumulative_sum\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCumulative_Sum_UDF\u001b[39m\u001b[38;5;124m\"\u001b[39m, cumulative_sum(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVendas\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Mostrar o DataFrame resultante\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[43mdf_with_cumulative_sum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Encerrar a sessão Spark\u001b[39;00m\n\u001b[1;32m     41\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_2484272/3170025621.py\", line 32, in cumulative_sum\n  File \"/home/Matheus/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\n  File \"/home/Matheus/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 866, in sum\n    return _invoke_function_over_columns(\"sum\", col)\n  File \"/home/Matheus/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 105, in _invoke_function_over_columns\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n  File \"/home/Matheus/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 105, in <genexpr>\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n  File \"/home/Matheus/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 65, in _to_java_column\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got int.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, sum, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Inicialize a sessão Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Exemplo de UDF em Window\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Crie um DataFrame de exemplo\n",
    "data = [\n",
    "    (1, \"Loja_A\", 100),\n",
    "    (2, \"Loja_A\", 150),\n",
    "    (3, \"Loja_A\", 200),\n",
    "    (4, \"Loja_B\", 300),\n",
    "    (5, \"Loja_B\", 250),\n",
    "    (6, \"Loja_B\", 200),\n",
    "    (7, \"Loja_B\", 350)\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"ID\", \"Loja\", \"Vendas\"])\n",
    "\n",
    "# Defina a janela de partição por loja e ordene pelos IDs\n",
    "windowSpec = Window.partitionBy(\"Loja\").orderBy(\"ID\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "# Adicione uma coluna com a soma acumulada das vendas por loja\n",
    "df_with_cumulative_sum = df.withColumn(\"Cumulative_Sum\", sum(col(\"Vendas\")).over(windowSpec))\n",
    "\n",
    "# Defina uma UDF para calcular a soma acumulada\n",
    "@udf(IntegerType())\n",
    "def cumulative_sum(column):\n",
    "    return sum(column) if column is not None else None\n",
    "\n",
    "# Aplicar a UDF à coluna de vendas\n",
    "df_with_cumulative_sum = df_with_cumulative_sum.withColumn(\"Cumulative_Sum_UDF\", cumulative_sum(col(\"Vendas\")))\n",
    "\n",
    "# Mostrar o DataFrame resultante\n",
    "df_with_cumulative_sum.show()\n",
    "\n",
    "# Encerrar a sessão Spark\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
